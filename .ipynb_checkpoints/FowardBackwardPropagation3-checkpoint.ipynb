{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16f9b918-099a-4137-99f3-1abd127d7dba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T19:59:39.865139Z",
     "iopub.status.busy": "2026-02-05T19:59:39.863944Z",
     "iopub.status.idle": "2026-02-05T19:59:42.259206Z",
     "shell.execute_reply": "2026-02-05T19:59:42.257203Z",
     "shell.execute_reply.started": "2026-02-05T19:59:39.865077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipynb\n",
      "  Downloading ipynb-0.5.1-py3-none-any.whl.metadata (303 bytes)\n",
      "Downloading ipynb-0.5.1-py3-none-any.whl (6.9 kB)\n",
      "Installing collected packages: ipynb\n",
      "Successfully installed ipynb-0.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325b9cab-9b18-4ee5-b86a-cf6d990a2852",
   "metadata": {},
   "source": [
    "In order to import the notebook, like how we import a package, which is a .py file in a regular IDE.\n",
    "We will be using from ipynb.fs.full.<notebook_name> import <function_name>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c5f5a39-203c-48ed-8777-40e33fd5930d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T20:03:34.630834Z",
     "iopub.status.busy": "2026-02-05T20:03:34.629903Z",
     "iopub.status.idle": "2026-02-05T20:03:34.636853Z",
     "shell.execute_reply": "2026-02-05T20:03:34.635177Z",
     "shell.execute_reply.started": "2026-02-05T20:03:34.630765Z"
    }
   },
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import ActivationFunctions2\n",
    "from ActivationFunctions2 import ActivationFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "066d47c0-bd0b-4b9e-a317-397059c2e74f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T19:50:37.204444Z",
     "iopub.status.busy": "2026-02-05T19:50:37.203936Z",
     "iopub.status.idle": "2026-02-05T19:50:37.214651Z",
     "shell.execute_reply": "2026-02-05T19:50:37.213326Z",
     "shell.execute_reply.started": "2026-02-05T19:50:37.204404Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ca1a3d4-5872-427a-888d-cda2134a0f46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T19:51:07.900431Z",
     "iopub.status.busy": "2026-02-05T19:51:07.900015Z",
     "iopub.status.idle": "2026-02-05T19:51:07.909196Z",
     "shell.execute_reply": "2026-02-05T19:51:07.908009Z",
     "shell.execute_reply.started": "2026-02-05T19:51:07.900394Z"
    }
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, activation_fn, activation_dfn):\n",
    "        # We use \"He Initialization\" (random numbers scaled) \n",
    "        # because it helps the model learn faster.\n",
    "        # Instead of * 0.1, use this formula (Xavier Initialization):\n",
    "        self.W = np.random.randn(output_size, input_size) * np.sqrt(1. / input_size)\n",
    "        self.b = np.zeros((output_size, 1))\n",
    "        \n",
    "        self.activation_fn = activation_fn\n",
    "        self.activation_dfn = activation_dfn\n",
    "        \n",
    "        # We need to store these for the backward pass\n",
    "        self.A_prev = None\n",
    "        self.Z = None\n",
    "\n",
    "    def forward(self, A_prev):\n",
    "        self.A_prev = A_prev\n",
    "        # Z = W . A_prev + b\n",
    "        self.Z = np.dot(self.W, A_prev) + self.b\n",
    "        # A = activation(Z)\n",
    "        self.A = self.activation_fn(self.Z)\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dA, learning_rate):\n",
    "        # dA is the 'error signal' coming from the next layer\n",
    "        \n",
    "        # 1. Calculate how Z affected the loss (dZ)\n",
    "        # dZ = dA * activation_derivative(Z)\n",
    "        dZ = dA * self.activation_dfn(self.Z)\n",
    "        \n",
    "        # 2. Calculate how W affected the loss (dW)\n",
    "        # dW = dZ . A_prev_transpose\n",
    "        m = self.A_prev.shape[1]\n",
    "        dW = np.dot(dZ, self.A_prev.T) / m\n",
    "        db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "        \n",
    "        # 3. Calculate error for the PREVIOUS layer (dA_prev)\n",
    "        dA_prev = np.dot(self.W.T, dZ)\n",
    "        \n",
    "        # 4. Update Weights and Biases (Gradient Descent)\n",
    "        self.W -= learning_rate * dW\n",
    "        self.b -= learning_rate * db\n",
    "        \n",
    "        return dA_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fb7752-fc25-4880-b88e-f62fba232b91",
   "metadata": {},
   "source": [
    "### 3. Why did we do it this way? (Mastery Explanation)\n",
    "A_prev: In the backward pass, to update the weight between Layer 1 and Layer 2, you need to know what the input was during the forward pass. Thatâ€™s why we \"cache\" (store) A_prev.\n",
    "dA_prev: Notice the function returns dA_prev. This is how the \"Chain\" works. Layer 3 calculates its error and hands it back to Layer 2. Layer 2 uses it and hands its error back to Layer 1.\n",
    "axis=1, keepdims=True: In the bias update (db), we sum up all the errors for a whole batch of data. keepdims ensures the shape stays \n",
    "(\n",
    "n\n",
    ",\n",
    "1\n",
    ")\n",
    "(n,1)\n",
    " so we don't accidentally turn a column into a flat list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7004a19-c6ae-41b7-9705-84dff5dfcaa0",
   "metadata": {},
   "source": [
    "### 4. Mathematical Logic (The \"Aha!\" Moment)\n",
    "In an interview, they might ask: \"What is the derivative of the weights?\"\n",
    "\n",
    "The answer is: Inputs * Error Signal\n",
    "\n",
    "If the input was 0, the weight had no effect on the error, so the gradient is 0.\n",
    "If the input was huge, the weight had a huge effect, so the gradient is large.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c30b96e5-414f-4c90-b594-cfb41073d7f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T20:03:37.908260Z",
     "iopub.status.busy": "2026-02-05T20:03:37.907860Z",
     "iopub.status.idle": "2026-02-05T20:03:37.917787Z",
     "shell.execute_reply": "2026-02-05T20:03:37.915889Z",
     "shell.execute_reply.started": "2026-02-05T20:03:37.908225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of Forward Pass:\n",
      " [[0.       ]\n",
      " [0.0891017]]\n",
      "\n",
      "Error passed back to input:\n",
      " [[ 0.01081987]\n",
      " [ 0.01739602]\n",
      " [-0.00880316]]\n"
     ]
    }
   ],
   "source": [
    "# Use the activation functions we wrote in Notebook 2\n",
    "# Let's assume we have 3 input features, and we want 2 hidden neurons\n",
    "layer1 = Layer(3, 2, ActivationFunction.relu, ActivationFunction.relu_derivative)\n",
    "\n",
    "# Mock input data (3 features, 1 sample)\n",
    "x_sample = np.array([[0.5], [0.1], [-0.2]])\n",
    "\n",
    "# Run forward pass\n",
    "output = layer1.forward(x_sample)\n",
    "print(\"Output of Forward Pass:\\n\", output)\n",
    "\n",
    "# Run backward pass (Assume error dA from next layer is 0.1)\n",
    "dA_mock = np.array([[0.1], [0.1]])\n",
    "prev_error = layer1.backward(dA_mock, learning_rate=0.01)\n",
    "print(\"\\nError passed back to input:\\n\", prev_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40beccf6-b11b-4b9d-b834-fb96dbdd1048",
   "metadata": {},
   "source": [
    "Once you run this, I have a question: In the Layer class, we initialized W with np.random.randn. \n",
    "What would happen if we initialized all weights to Zero? (This is a very famous interview question!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc2d53d-3522-4a3c-ace3-3d1fdb6ff60b",
   "metadata": {},
   "source": [
    "### The Answer to the \"Zero Initialization\" Question:\n",
    "If you initialize all weights to Zero, you run into the Symmetry Problem.\n",
    "\n",
    "1. Identical Output: Every neuron in the hidden layer will receive the same input and have the same weight (0). Therefore, they will all calculate the exact same output.\n",
    "2. Identical Gradient: During backprop, every neuron will receive the exact same gradient update.\n",
    "3. No Diversity: Even after 1,000 epochs of training, every neuron in that layer will still be identical to its neighbor.\n",
    "4. Result: Your \"Neural Network\" effectively becomes a single-neuron model because all neurons are doing the same thing.\n",
    "   \n",
    "Conclusion: We use random numbers (like np.random.randn) to \"Break Symmetry.\" It gives each neuron a different \"starting perspective\" so they can learn different features (e.g., one neuron learns to find edges, another finds circles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f900fb-1409-4811-ac7e-d68d2e2a0901",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m139",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m139"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
