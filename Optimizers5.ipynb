{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e96b203-36d2-4c27-aa2c-149f3c70a56d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T20:25:40.413966Z",
     "iopub.status.busy": "2026-02-05T20:25:40.413554Z",
     "iopub.status.idle": "2026-02-05T20:25:40.423995Z",
     "shell.execute_reply": "2026-02-05T20:25:40.422323Z",
     "shell.execute_reply.started": "2026-02-05T20:25:40.413927Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7caf29ad-abc0-4588-b9b6-fd8e80104365",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T20:25:57.202038Z",
     "iopub.status.busy": "2026-02-05T20:25:57.201634Z",
     "iopub.status.idle": "2026-02-05T20:25:57.217047Z",
     "shell.execute_reply": "2026-02-05T20:25:57.215429Z",
     "shell.execute_reply.started": "2026-02-05T20:25:57.201979Z"
    }
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
    "        self.lr = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.velocities = {} # To store momentum for each weight/bias\n",
    "\n",
    "    def update(self, layer_id, W, dW, b, db):\n",
    "        # Initialize velocity for this layer if not exists\n",
    "        if layer_id not in self.velocities:\n",
    "            self.velocities[layer_id] = {'W': np.zeros_like(W), 'b': np.zeros_like(b)}\n",
    "        \n",
    "        v = self.velocities[layer_id]\n",
    "        \n",
    "        # Calculate Momentum: v = momentum * v - lr * gradient\n",
    "        v['W'] = self.momentum * v['W'] - self.lr * dW\n",
    "        v['b'] = self.momentum * v['b'] - self.lr * db\n",
    "        \n",
    "        # Update parameters\n",
    "        new_W = W + v['W']\n",
    "        new_b = b + v['b']\n",
    "        \n",
    "        return new_W, new_b\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.lr = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = {} # First moment (mean of gradients)\n",
    "        self.v = {} # Second moment (uncentered variance)\n",
    "        self.t = 0  # Time step\n",
    "\n",
    "    def update(self, layer_id, W, dW, b, db):\n",
    "        if layer_id not in self.m:\n",
    "            self.m[layer_id] = {'W': np.zeros_like(W), 'b': np.zeros_like(b)}\n",
    "            self.v[layer_id] = {'W': np.zeros_like(W), 'b': np.zeros_like(b)}\n",
    "        \n",
    "        self.t += 1\n",
    "        \n",
    "        # Update for Weight (W)\n",
    "        self.m[layer_id]['W'] = self.beta1 * self.m[layer_id]['W'] + (1 - self.beta1) * dW\n",
    "        self.v[layer_id]['W'] = self.beta2 * self.v[layer_id]['W'] + (1 - self.beta2) * (dW**2)\n",
    "        \n",
    "        # Bias correction (makes learning stable at the very start)\n",
    "        m_hat = self.m[layer_id]['W'] / (1 - self.beta1**self.t)\n",
    "        v_hat = self.v[layer_id]['W'] / (1 - self.beta2**self.t)\n",
    "        \n",
    "        new_W = W - self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "        \n",
    "        # (Same logic for bias b... simplified here for brevity)\n",
    "        new_b = b - self.lr * db \n",
    "        \n",
    "        return new_W, new_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4872d2e-03aa-4731-ab68-bd3e525e7266",
   "metadata": {},
   "source": [
    "### Why Adam?\n",
    "\"Why did you use Adam for your project?\"\n",
    "1. Adaptive Learning Rates: I don't have to spend weeks tuning the learning rate because Adam adjusts it for me.\n",
    "2. Handles Sparse Gradients: If some features are rare (like certain words in text), Adam handles them better than SGD.\n",
    "3. Faster Convergence: It usually reaches a low loss much faster than standard Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c6eb8c-6b21-42c2-aa79-e92075eb866b",
   "metadata": {},
   "source": [
    "Compare the SGD update and the Adam update.\n",
    "Notice how Adam uses dW**2 (the squared gradient).\n",
    "Question for you: If a weight has a very large and shaky gradient, what will happen to the value of v_hat (the variance), and how will that affect the size of the weight update?\n",
    "(Hint: Look at the line new_W = W - lr * m_hat / np.sqrt(v_hat). If v_hat is big, what happens to the division?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca889ed-59b0-4958-ad51-2fb74936c36b",
   "metadata": {},
   "source": [
    "If a weight has a large and shaky gradient, the value of v_hat(the variance) will become very large.\n",
    "The result:\n",
    "The total weight update becomes smaller.\n",
    "Why is this brilliant?\n",
    "Adam acts like an automatic braking system. If a weight is oscillating wildly (shaky gradients), Adam thinks, \"Whoa, this weight is unstable, let's take smaller steps.\" If a weight has a small, consistent gradient, Adam thinks, \"This path is safe, let's speed up,\" and the small denominator makes the step size larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd82cffa-b74e-4f20-ba13-77a9e03bc3e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T20:28:16.035426Z",
     "iopub.status.busy": "2026-02-05T20:28:16.034086Z",
     "iopub.status.idle": "2026-02-05T20:28:16.046542Z",
     "shell.execute_reply": "2026-02-05T20:28:16.044879Z",
     "shell.execute_reply.started": "2026-02-05T20:28:16.035364Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam step for Stable Neuron: 0.000996\n",
      "Adam step for Shaky Neuron:  0.000204\n",
      "\n",
      "Observation: Even though the shaky gradients were 100x larger,\n",
      "Adam kept the step size controlled!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simulate_adam_step(gradient_history, lr=0.001):\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    m = 0 # First moment\n",
    "    v = 0 # Second moment\n",
    "    \n",
    "    # Simulate 10 time steps of training\n",
    "    for t, dW in enumerate(gradient_history, 1):\n",
    "        m = beta1 * m + (1 - beta1) * dW\n",
    "        v = beta2 * v + (1 - beta2) * (dW**2)\n",
    "        \n",
    "        # Bias correction\n",
    "        m_hat = m / (1 - beta1**t)\n",
    "        v_hat = v / (1 - beta2**t)\n",
    "        \n",
    "        # The effective step size\n",
    "        step = lr * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "        \n",
    "    return step\n",
    "\n",
    "# Case 1: The Stable Neuron (Gradients are consistently small)\n",
    "stable_grads = [0.1, 0.11, 0.09, 0.1, 0.1]\n",
    "step_stable = simulate_adam_step(stable_grads)\n",
    "\n",
    "# Case 2: The Shaky Neuron (Gradients are huge and jumping)\n",
    "shaky_grads = [10.0, -10.0, 10.0, -10.0, 10.0]\n",
    "step_shaky = simulate_adam_step(shaky_grads)\n",
    "\n",
    "print(f\"Adam step for Stable Neuron: {step_stable:.6f}\")\n",
    "print(f\"Adam step for Shaky Neuron:  {step_shaky:.6f}\")\n",
    "print(f\"\\nObservation: Even though the shaky gradients were 100x larger,\")\n",
    "print(f\"Adam kept the step size controlled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5fa77c-38e0-43ef-b74b-2b500e3703f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m139",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m139"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
